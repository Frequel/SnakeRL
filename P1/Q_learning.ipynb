{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.7.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, QLearningAgent\n",
    "from environment import Environment\n",
    "from screen import Screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurações gerais para a execução dos modelos:\n",
    "\n",
    "* O espaço de ações é denifido como uma matriz identidade 3x3\n",
    "* Há três ações possíveis (continuar na mesma direção, virar para a esquerda, virar para a direita)\n",
    "* Como cada estado é representado por um vetor binário de 11 posições, ao todo há 2^11 estados possíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de recompensa padrão é definida como:\n",
    "    \n",
    "* Ganha 10 pontos por comer a maçã\n",
    "* Perde 10 pontos por morrer\n",
    "* Recompensa 0 caso contrário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_learning(agent: Agent, reward_function, episodes, display, speed, verbose=True):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "                    \n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            decoded_action1 = decode_action(action1)\n",
    "            decoded_action2 = decode_action(action2)\n",
    "            agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "            \n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "        \n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 1: Baseline\n",
    "\n",
    "* N0 = 1\n",
    "* gamma = 1\n",
    "* número de episódios = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1      Score: 0\n",
      "Game 2      Score: 0\n",
      "Game 3      Score: 0\n",
      "Game 4      Score: 1\n",
      "Game 5      Score: 1\n",
      "Game 6      Score: 2\n",
      "Game 7      Score: 1\n",
      "Game 8      Score: 0\n",
      "Game 9      Score: 1\n",
      "Game 10      Score: 1\n",
      "Game 11      Score: 1\n",
      "Game 12      Score: 1\n",
      "Game 13      Score: 1\n",
      "Game 14      Score: 1\n",
      "Game 15      Score: 1\n",
      "Game 16      Score: 1\n",
      "Game 17      Score: 1\n",
      "Game 18      Score: 1\n",
      "Game 19      Score: 1\n",
      "Game 20      Score: 2\n",
      "Game 21      Score: 3\n",
      "Game 22      Score: 2\n",
      "Game 23      Score: 3\n",
      "Game 24      Score: 2\n",
      "Game 25      Score: 2\n",
      "Game 26      Score: 2\n",
      "Game 27      Score: 1\n",
      "Game 28      Score: 1\n",
      "Game 29      Score: 2\n",
      "Game 30      Score: 2\n",
      "Game 31      Score: 1\n",
      "Game 32      Score: 1\n",
      "Game 33      Score: 3\n",
      "Game 34      Score: 3\n",
      "Game 35      Score: 3\n",
      "Game 36      Score: 4\n",
      "Game 37      Score: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-a2996bf0ec3f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_q_learning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mqLearningAgent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward_function\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdefault_reward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepisodes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspeed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mend\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-73c4f25f2146>\u001B[0m in \u001B[0;36mrun_q_learning\u001B[0;34m(agent, reward_function, episodes, display, speed, verbose)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m                 \u001B[0mscreen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m                 \u001B[0mpygame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspeed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mestrado/reinforcement_learning/snake/screen.py\u001B[0m in \u001B[0;36mdisplay\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameDisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m255\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m255\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m255\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__display_ui\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m         self.env.player.display_player(self.env.player.position[-1][0], self.env.player.position[-1][1],\n\u001B[1;32m     34\u001B[0m                                        self.env.player.food, self.env.game)\n",
      "\u001B[0;32m~/mestrado/reinforcement_learning/snake/screen.py\u001B[0m in \u001B[0;36m__display_ui\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mmyfont\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpygame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSysFont\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Segoe UI'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0mmyfont_bold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpygame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSysFont\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Segoe UI'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m         \u001B[0mtext_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmyfont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'SCORE: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m         \u001B[0mtext_score_number\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmyfont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0mtext_highest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmyfont\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'HIGHEST SCORE: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=200, speed=0, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.95\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como no treinamento do modelo com ``gamma = 0.98`` o score apresenta uma tendência de crescimento que é maior que as demais, tomaremos 0.98 como o fator de desconto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.75\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.25\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 2\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 4\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 10\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 3: Variando a função de recompensa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward1(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -1 otherwise\n",
    "    \"\"\"\n",
    "    reward = -1\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "\n",
    "    return reward\n",
    "    \n",
    "\n",
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=reward1, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_exp(env, max_value, max_dist=3, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        +max_value * exp(dist[snake, food]) if dist[snake, food] <= max_dist\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    \n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = dist_metric(player_pos, food_pos)\n",
    "    \n",
    "    reward = max_value * math.exp(dist)\n",
    "    if env.game.crash:\n",
    "        reward = -1000\n",
    "    elif env.player.eaten:\n",
    "        reward = 1000\n",
    "        non_improvement = 0\n",
    "    elif dist >= max_dist:\n",
    "        reward = -non_improvement*0.01\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_exp(env, max_value=10),\n",
    "                         episodes=1000, speed=1, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_linear(env, penalty_rate=0.01, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    reward = -non_improvement*penalty_rate\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "        non_improvement = 0\n",
    "    else:\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=reward_linear,\n",
    "                         episodes=1000, speed=1, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.02),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.05),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.1),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.2),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.5),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuração que resultou na melhor pontuação média para os últimos 50 episódios foi `N0 = 0.5`\n",
    "`gamma = 0.98` e `penalty_rate = 0.05`. Assim, a cobra atingiu o tamanho 72, tendo tamanho médio nos últimos 50 episódios igual a 41."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
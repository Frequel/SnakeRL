{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Objetivos do Trabalho\n",
    "O objetivo deste trabalho é aplicar métodos de aprendizado por reforço a um problema escolhido e modelado pelos estudantes, comparando os resultados obtidos pelos métodos Monte Carlo, Q-Learning e SARSA(lambda), avaliando: qualidade das soluções, custo computacional, tamanho do espaço de estados, tamanho do espaço de ações e influência da função de reforço no resultado. Também pretende-se aplicar aproximadores lineares de função a cada método e comparar o desempenho de cada método **com** e **sem** o aproximador de função.\n",
    "\n",
    "# Membros do Grupo e Contribuições\n",
    "\n",
    "### Ana Clara Zoppi Serpa (RA 165880)\n",
    "- Implementação do método Monte Carlo\n",
    "- Implementação do método SARSA(lambda)\n",
    "- Escrita do relatório\n",
    "- Formulação do problema como MDP\n",
    "- Análise de resultados\n",
    "\n",
    "### Gabriel Oliveira dos Santos (RA 197460)\n",
    "- Implementação do método Q-Learning\n",
    "- Implementação do método SARSA\n",
    "- Experimentos com diferentes funções de reforço, nos métodos Q-Learning e SARSA\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "### Silvio Bento Garcia Junior (RA 265194)\n",
    "- Melhorias nos experimentos com o método de Monte Carlo, explorando diferentes funções de reforço\n",
    "- Realização de experimentos com os métodos SARSA(lambda) e com o Aproximador Linear de Função em todos os métodos\n",
    "- Formulação do problema como MDP\n",
    "- Construção das tabelas com resultados experimentais\n",
    "\n",
    "### Tito Barbosa Rezende (RA 025327)\n",
    "- Implementação dos métodos Monte Carlo e SARSA(lambda)\n",
    "- Implementação do Aproximador Linear de Função para todos os métodos\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "# Problema\n",
    "Escolhemos trabalhar com o jogo Snake, também conhecido como \"Serpente\" ou \"Jogo da Cobrinha\", disponível em https://www.google.com/fbx?fbx=snake_arcade.\n",
    "\n",
    "O jogador é uma cobra, inicialmente pequena, e seu objetivo é comer maçãs. A cobra pode se mover para cima, para baixo, para a esquerda e para a direita. Maçãs aparecem em posições aleatórias do mapa, e o jogador deve se movimentar a fim de passar pela posição que contém a maçã atual. O jogador pode se mover livremente, mas não pode colidir com as bordas do mapa (paredes), nem com seu próprio corpo. Conforme as maçãs são consumidas, o tamanho da cobra aumenta, e há mais risco de colidir com seu próprio corpo ao se movimentar pelo mapa, assim aumentando a dificuldade do jogo.\n",
    "\n",
    "## Formulação do problema como MDP\n",
    "\n",
    "Conforme visto em aula, um Processo Decisório de Markov (MDP) é uma tupla <S, P, A, R, gamma>, sendo S um conjunto finito de estados, A um conjunto finito de ações, P uma matriz de probabilidades de transições entre estados, R uma função de reforço e gamma um fator de desconto, que é um número real no intervalo [0,1].\n",
    "\n",
    "Um estado é uma tupla com 11 valores booleanos:\n",
    "1. Se há perigo (parede ou o próprio corpo da cobra) uma ou duas unidades do tabuleiro à frente;\n",
    "2. Se há perigo uma ou duas unidades à direita;\n",
    "3. Se há perigo uma ou duas unidades à esquerda;\n",
    "4. Se a cobra está se movendo para a esquerda;\n",
    "5. Se a cobra está se movendo para a direita;\n",
    "6. Se a cobra está se movendo para cima;\n",
    "7. Se a cobra está se movendo para baixo.\n",
    "8. Se há comida à esquerda;\n",
    "9. Se há comida à direita;\n",
    "10. Se há comida para cima;\n",
    "11. Se há comida para baixo;\n",
    "\n",
    "Há 2^11 estados possíveis.\n",
    "\n",
    "Essas direções (esquerda, direita, cima, baixo) são sempre consideradas na perspectiva da cabeça da cobra.\n",
    "\n",
    "As ações possíveis são:\n",
    "- Continuar indo na mesma direção, representada pelo vetor [1,0,0];\n",
    "- Virar para a direita, representada por [0,1,0];\n",
    "- Virar para a esquerda, representada por [0,0,1];\n",
    "\n",
    "A tela de jogo tem 440 pixels de largura e 440 pixels de altura. Cada parte do corpo da cobra é um quadrado de 20 pixels de altura e 20 pixels de largura. A maçã tem 20 pixels de altura e 20 pixels de largura também.\n",
    "\n",
    "A matriz de transição de probabilidades P contém somente uns e zeros, pois dada uma posição (estado) atual no tabuleiro e uma ação, o resultado é sempre o mesmo. Para cada estado atual **s**, estado resultante **s'** quando tomada uma ação **a** e estados restantes **Z** que não são obtidos tomando **a** a partir de **s**, a probabilidade de ir de **s** para **s'** é 1, enquanto que de **s** a qualquer dos **Z** é zero. Isso vale para todos os estados e ações, por isso a matriz terá somente zeros e uns.\n",
    "\n",
    "Quanto à função de reforço, definimos uma função padrão e depois exploramos outras. Isso será detalhado mais adiante no relatório, assim como os diferentes fatores de desconto (gamma) adotados nos experimentos.\n",
    "\n",
    "## Natureza do ambiente\n",
    "\n",
    "* Determinístico ou estocástico: **determinístico**, pois, dado um estado corrente e uma ação, o estado resultante é sempre o mesmo (dada uma posição, a ação de continuar na mesma direção/virar para a direita/virar para a esquerda leva sempre ao mesmo destino).\n",
    "\n",
    "* Contínuo ou discreto: **discreto**, já que temos um conjunto finito de ações que podemos escolher tomar a partir de um certo estado (virar para a direita, virar para a esquerda, continuar na mesma direção).\n",
    "\n",
    "* Episódico ou não episódico: **episódico**, sendo cada episódio um jogo. O episódio termina somente quando a cobra morre.\n",
    "\n",
    "* Single-agent ou multi-agent: **single-agent**, pois nosso único agente é a cobra.\n",
    "\n",
    "## Modelo de discretização adotado\n",
    "\n",
    "Como nosso ambiente já é discreto, não precisamos adotar nenhuma estratégia de discretização.\n",
    "\n",
    "## Estados terminais\n",
    "\n",
    "Nossos estados terminais são aqueles referentes à morte da cobra, portanto ocorrem somente auando a cobra colide com a parede e quando a cobra colide com seu próprio corpo.\n",
    "\n",
    "## Funções de Reforço\n",
    "\n",
    "Nossa **função de reforço padrão** foi -10 quando a cobra morre, +10 quando come a maçã e 0 caso contrário.\n",
    "\n",
    "```\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "Exploramos também as seguintes funções de reforço alternativas:\n",
    "\n",
    "* Função **reward1**: visa penalizar a cobra por não estar comendo a maçã.\n",
    "\n",
    "```\n",
    "def reward1(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -1 otherwise\n",
    "    \"\"\"\n",
    "    reward = -1\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "* Função **reward_linear**: visa penalizar a cobra por cada momento em que não melhorar seu desempenho. É possível fornecer diferentes penalizações com o parâmetro penalty_rate.\n",
    "\n",
    "```\n",
    "def reward_linear(env, penalty_rate=0.01, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    reward = -non_improvement*penalty_rate\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "        non_improvement = 0\n",
    "    else:\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "* Função **reward_euclidean**: visa penalizar a cobra quando está longe da maçã, e recompensá-la quando está perto. Implementamos essa função de reforço numa tentativa de melhorar nossos resultados com o algoritmo de Monte Carlo.\n",
    "\n",
    "```\n",
    "def reward_euclidean(env):\n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = distance.euclidean(player_pos, food_pos)\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10000\n",
    "    elif env.player.eaten:\n",
    "        reward = 10000\n",
    "    else:\n",
    "        if dist > 100:\n",
    "            reward = (-1*dist)/10\n",
    "        if dist <= 100:\n",
    "            reward = (100-dist)/10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "## Política\n",
    "\n",
    "A política usada em todos os experimentos foi a **epsilon-greedy**, na qual o agente escolhe suas ações de forma gulosa com respeito à **função ação-valor**. A função ação-valor indica qual é o reforço esperado ao realizar certa ação, estando num certo estado. Dessa forma, agindo gulosamente com respeito à função ação-valor, o agente escolherá ações para as quais ele imagina que receberá a maior quantidade possível de reforço positivo.\n",
    "\n",
    "No entanto, para garantir que o agente explore outras ações em vez de realizar sempre as mesmas escolhas, com probabilidade epsilon uma ação aleatória será escolhida em vez da ação gulosa.\n",
    "\n",
    "Os métodos Monte Carlo e SARSA são métodos **on-policy** pois neles a política que o agente busca melhorar é a própria política que ele está utilizando para interagir com o ambiente, no caso, a própria epsilon-greedy. Já o método Q-Learning se caracteriza como **off-policy** pois nele o agente se comporta conforme a epsilon-greedy, mas está procurando aprender outra política.\n",
    "\n",
    "## Parâmetros\n",
    "\n",
    "Exploramos os parâmetros gamma, alfa e N0 em nossos experimentos.\n",
    "\n",
    "* Fator de desconto (gamma): regula o quanto o agente prioriza reforços futuros. Se gamma for baixo, o desconto de reforços futuros é baixo, e o agente dará atenção a possíveis reforços futuros. Se gamma for alto, o desconto será alto, e o agente se comportará priorizando reforços de curto prazo.\n",
    "\n",
    "* Passo de aprendizado / \"step-size\" (alfa): regula o ajuste que é realizado na função ação-valor conforme o agente interage com o ambiente, ou seja, o quanto ele aprende com a diferença entre reforços reais e reforços que ele imaginava que receberia com seu comportamento.\n",
    "\n",
    "* N0: constante utilizada para inicialização dos métodos. É usada no cálculo da probabilidade epsilon da política epsilon-greedy (epsilon = N0/(N0 + N(s_t), sendo N(s_t) o número de visitas a um certo estado s, no instante t).\n",
    "\n",
    "\n",
    "# Pontos-chave da implementação\n",
    "\n",
    "### Arquivo agent.py\n",
    "\n",
    "Neste arquivo há uma classe Agent, que serve como base para as demais (QLearningAgent, MonteCarloAgent, e assim por diante). Ela possui os métodos `choose_action` e `update`. O método `choose_action` é a política, ou seja, escolhe uma ação dado um estado de forma epsilon-greedy. O método `update` é deixado vazio nessa classe, pois cada método tem sua forma de atualizar a função ação-valor (atributo `Q` da classe).\n",
    "\n",
    "Logo depois da classe Agent, temos as classes específicas para cada método. Um exemplo disso é a classe QLearningAgent, com sua implementação do método update:\n",
    "\n",
    "```\n",
    "def update(self, prev_state, next_state, reward, prev_action, next_action):\n",
    "        \"\"\"\n",
    "        Update the action value function using the Q-Learning update.\n",
    "        Q(S_t, A_t) = Q(S_t, A_t) + alpha(reward + (gamma * Max Q(S_t+1, *) - Q(S_t, A_t))\n",
    "        Args:\n",
    "            prev_state: The previous state\n",
    "            next_state: The next state\n",
    "            reward: The reward for taking the respective action\n",
    "            prev_action: The previous action\n",
    "            next_action: The next action\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        alpha = 1 / self.state_action_counter[prev_state, prev_action]\n",
    "        predict = self.Q[prev_state, prev_action]\n",
    "        target = reward + self.gamma * np.max(self.Q[next_state, :])\n",
    "        self.Q[prev_state, prev_action] += alpha * (target - predict)\n",
    "```\n",
    "\n",
    "As outras classes (SARSAAgent, SARSALambdaAgent) foram implementadas similarmente, preenchendo o método `update` com a regra de atualização adequada. A classe MonteCarloAgent é uma exceção --- não implementa o método `update` pois a atualização da função ação-valor é realizada diretamente no método que executa os episódios.\n",
    "\n",
    "### Arquivo base_classes.py\n",
    "\n",
    "Neste arquivo estão classes relacionadas aos recursos gráficos do jogo e sua execução com a biblioteca PyGame: atualizar posição da cobra na tela, exibir maçãs em novos locais, etc.\n",
    "\n",
    "### Arquivo environment.py\n",
    "\n",
    "Temos neste arquivo a classe Environment, com funções `reset`, `step` e `__get_state`. A função `reset` recomeça o jogo uma vez que ele tenha terminado. A função `step` move a cobra conforme a ação que foi escolhida e retorna qual o novo estado e o reforço resultante disso. A função `__get_state` informa o estado atual --- a tupla de 11 booleanos, conforme descrito anteriormente.\n",
    "\n",
    "### Funções para executar os métodos de aprendizado\n",
    "\n",
    "Para executar cada método de aprendizado, foram definidas funções no Jupyter Notebook: `run_q_learning`, `run_sarsa`, `run_monte_carlo`. Aqui é exibida como exemplo `run_q_learning`.\n",
    "\n",
    "- A seed é configurada.\n",
    "- O ambiente é inicializado.\n",
    "- Nosso dicionário de métricas é inicializado vazio.\n",
    "- Guardamos o instante no qual a execução começou.\n",
    "- Começamos os episódios.\n",
    "- Um episódio começa com a chamada a env.reset().\n",
    "- Um episódio está contido no loop `while not done`.\n",
    "- Dentro da execução do episódio atual, vemos qual o estado e o reforço após a execução da ação 1 escolhida ao iniciarmos o episódio.\n",
    "- Atualizamos os valores da função ação-valor com uma chamada a `agent.update`.\n",
    "- Acumulamos o reforço no contador de reforço do episódio corrente.\n",
    "- Isso será repetido até o episódio acabar, então sempre será escolhida uma próxima ação, obtido um reforço, atualizados os valores `Q` da função valor.\n",
    "- Quando um episódio acaba, atualizamos as métricas.\n",
    "- Observação: caso um único jogo (episódio) passe de 10 minutos de duração, ele é interrompido para iniciarmos um novo.\n",
    "- Quanto todos os episódios acabam, retornamos as métricas.\n",
    "\n",
    "```\n",
    "def run_q_learning(agent: Agent, reward_function, episodes, display, speed, verbose=True):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "                    \n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            decoded_action1 = decode_action(action1)\n",
    "            decoded_action2 = decode_action(action2)\n",
    "            agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "            \n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "        \n",
    "\n",
    "    return metrics\n",
    "```\n",
    "\n",
    "### Realização de experimentos\n",
    "\n",
    "Para realizar um experimento com certo método, definem-se os valores de N0 e gamma, cria-se um agente do tipo desejado e chama-se a função `run_` correspondente, passando como parâmetro a função de reforço, o número de episódios, a velocidade desejada para ver os movimentos da cobra e se o jogo será exibido ou não (variável `display`). Configurando `display` para False, os experimentos demoram menos tempo, então mantivemos `display` configurado como False para coletar os dados. Basta configurar `display=True` para visualizar o jogo.\n",
    "\n",
    "Como exemplo, segue um experimento que pode ser feito com Q-Learning. Os outros experimentos são análogos.\n",
    "```\n",
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=200, speed=5, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))\n",
    "```\n",
    "\n",
    "## Particularidades e restrições da implementação\n",
    "\n",
    "Para executar o código, é preciso instalar a biblioteca PyGame. Isso pode ser feito com o comando `pip install pygame` no terminal.\n",
    "\n",
    "# Métodos\n",
    "\n",
    "Os métodos implementados são métodos de controle, ou seja, que visam melhorar a política conforme o agente interage com o ambiente. Isso é feito por meio da atualização dos valores `Q(s,a)` (função ação-valor), que indicam o reforço que o agente espera receber escolhendo uma ação `a` quando está num estado `s`. Conforme o agente interage com o ambiente e realiza ações, ele recebe reforços reais e ajusta `Q(s,a)`. Como `Q(s,a)` é utilizada nas próximas escolhas, o que ocorre é que o agente mude seu comportamento (**política**) conforme a atualização de `Q(s,a)` e os reforços positivos ou negativos recebidos.\n",
    "\n",
    "Cada método difere em como atualiza `Q(s,a)`, produzindo diferentes resultados.\n",
    "\n",
    "## Monte Carlo\n",
    "\n",
    "### First-Visit\n",
    "\n",
    "Q(s,a) é inicializado com zeros, assim como N(s,a), que indicará o número de vezes que, no estado **s**, a ação **a** foi escolhida. Epsilon é inicializado com 1, k é inicializado com 1.\n",
    "\n",
    "Então, para cada episódio k, é feito o *sampling*: o episódio inteiro é executado seguindo a política, ou seja, começa-se num estado s_k_1, realiza-se a ação a_k_1, recebe-se o reforço r_k_1, chega-se ao estado s_k_2, e assim por diante, até o último estado do episódio, denotado por s_k_T.\n",
    "\n",
    "Uma vez terminado o episódio, calcula-se o retorno dele, guardando na variável G. O retorno é a soma dos reforços obtidos em cada estado, multplicados pelo fator de desconto gamma.\n",
    "\n",
    "Então, para cada período de 1 a T, atualiza-se N(s,a). No caso do *first-visit Monte Carlo*, N(s,a) será incrementado para contar as primeiras visitas ao estado **s** e ação **a** no episódio k. Também é atualizada a função ação-valor Q(s,a), somando ao valor atual a diferença entre o retorno ganho G_k_t e o retorno esperado Q_st_at, multiplicada pelo passo de aprendizado alfa = 1/N(s,a).\n",
    "\n",
    "Depois disso, a política é melhorada, usando a atualização de Q(s,a) que acabamos de realizar.\n",
    "\n",
    "![](montecarlo-first-visit-pseudocode.png)\n",
    "\n",
    "### Every-Visit\n",
    "\n",
    "No **every-visit Monte Carlo**, a diferença é que N(s,a) é incrementado considerando todas as visitas ao estado **s** e à ação **a**, em vez de somente a primeira visita. O **every-visit Monte Carlo** apresentou resultados melhores que o first-visit Monte Carlo neste trabalho.\n",
    "\n",
    "Com o método de Monte Carlo, a atualização de Q(s,a) ocorre somente quando um episódio é terminado, ou seja, um jogo inteiro ocorre, com a cobra seguindo a política atual, e somente então a política é melhorada para ser utilizada num próximo jogo.\n",
    "\n",
    "## SARSA\n",
    "\n",
    "Inicializa-se Q(s,a) arbitrariamente, exceto no caso de estados terminais, que precisam ter Q(s,a) = 0. Para cada episódio, inicializa-se o estado atual S e escolhe-se uma ação A usando a política derivada de Q. **Para cada passo de aprendizado dentro do episódio atual**, a ação A escolhida é executada, sendo obtido o reforço R e o novo estado S'. Então, escolhe-se A' a partir de S', usando a política, e atualiza-se Q.\n",
    "\n",
    "Na atualização de Q, utilizamos o passo de aprendizado alfa, o reforço R obtido, o valor Q(S', A') e o valor Q(S,A): ou seja, estamos atualizando Q(S,A) levando em conta o reforço obtido ao executar A e o reforço que imaginamos receber com Q(S', A').\n",
    "\n",
    "Atualiza-se o estado atual para S' e a ação atual para A'.\n",
    "\n",
    "Isso é feito dentro de cada episódio, até que todos os episódios acabem. A principal diferença do SARSA em relação ao Monte Carlo é o aprendizado (atualização de Q) ao longo dos passos do episódio, e não só depois que o episódio já acabou, por ser um método baseado em diferença temporal.\n",
    "\n",
    "![](sarsa-pseudocode.png)\n",
    "\n",
    "## SARSA(lambda)\n",
    "\n",
    "No SARSA(lambda), para cada episódio há um conjunto E(S,A) que será usado na atualização de Q. Para cada passo de aprendizado de um episódio, uma ação A é tomada, sendo obtido o reforço R e um novo estado S'. É escolhida A' a partir de S'. Então, calcula-se delta usando R, o fator de desconto gamma, Q(S', A') e Q(S, A). O valor delta é o reforço obtido executando A, mais o valor que se espera receber executando A', com o desconto temporal gamma, menos o valor que se esperava receber executando A.\n",
    "\n",
    "E(S,A) é incrementado. Depois, para cada estado s e ação a, atualiza-se Q usando o passo de aprendizado alfa, delta e o valor E(s,a). E(S,A) também é atualizado usando gamma, lambda e E(s,a).\n",
    "\n",
    "A tabela E(S,A), chamada de **traços de eligibilidade**, deve funcionar como uma ponte entre métodos de diferença temporal e métodos Monte Carlo. Trata-se de uma memória temporária da ocorrência de um evento, então quando ocorre um erro, somente os estados e ações elegíveis são \"culpados\" pelo erro.\n",
    "\n",
    "![](sarsa-lambda-pseudocode.png)\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "O Q-Learning é bastante similar ao SARSA, mas a sua forma de atualizar Q(S,A) é diferente. Em vez de utilizar Q(S', A'), utiliza o maior valor dentre os Q(S', a) para todas as ações **a**.\n",
    "\n",
    "![](qlearning-pseudocode.png)\n",
    "\n",
    "--- FALAR MAIS DO Q-LEARNING SE DER ---\n",
    "\n",
    "## Aproximador Linear de Função\n",
    "\n",
    "Os métodos discutidos armazenam Q(s,a) como uma tabela e atualizam suas entradas conforme o aprendizado. No entanto, para problemas com muitos estados e/ou ações, utilizar tabelas não é viável. Aproximadores de Função aproximam Q(s,a) com uma função parametrizável, então em vez de consultar os valores reais na tabela são usadas aproximações. Podem ser usadas quaisquer aproximações que se deseje, por exemplo, com Redes Neurais. Neste trabalho utilizamos um Aproximador Linear de Função (LFA).\n",
    "\n",
    "--- FALAR MAIS SOBRE ISSO SE DER, EXPLICANDO FEATURE VECTOR E TALS ---\n",
    "\n",
    "# Principais Experimentos realizados\n",
    "\n",
    "\n",
    "\n",
    "# Discussão de Resultados\n",
    "\n",
    "![](results/tabela-monte-carlo.png)\n",
    "\n",
    "![](results/tabela-qlearning.png)\n",
    "\n",
    "![](results/tabela-sarsa.png)\n",
    "\n",
    "Para o Q-Learning, sem Aproximador Linear de Função, identificamos dois casos que consideramos ótimos:\n",
    "- Com N0 = 0.5 e gamma = 0.98, utilizando a função de reforço reward_linear sem nenhuma penalização, a cobra obteve 72 como pontuação máxima e 35.1 como média das últimas 50 pontuações, com tempo de execução 74,5140 segundos;\n",
    "- Com N0 = 0.5 e gamma = 0.98, utilizando a função de reforço reward_linear com penalty_rate = 0.05, a cobra obteve 72 como pontuação máxima e 41.04 como média das últimas 50 pontuações, em 94.3793 segundos;\n",
    "\n",
    "Para o SARSA, sem Aproximador Linear de Função, também encontramos dois casos que consideramos ótimos:\n",
    "- N0 = 1, gamma = 1, função de reforço padrão (default_reward): a cobra obteve 70 como pontuação máxima e 30.96 como pontuação média dos 50 últimos jogos, em 30.8162 segundos;\n",
    "- N0 = 1, gamma = 0.99, função de reforço reward_linear com penalidade = 0.2: a cobra obteve 70 como pontuação máxima e 31.86 como pontuação média dos últimos jogos, em 33.8256 segundos;\n",
    "\n",
    "O Q-Learning e o SARSA apresentaram casos ótimos bem próximos em termos de pontuação máxima e média das últimas pontuações, mas o tempo de execução do SARSA foi bem menor. Isso sugere que os dois algoritmos são adequados para o problema, e que SARSA tem vantagem sobre Q-Learning na velocidade do aprendizado da cobra.\n",
    "\n",
    "Os valores de gamma usados foram bem próximos de 1 nestes dois casos, o que indica que priorizar recompensas imediatas (\"pensar a curto prazo\" em vez de \"pensar a longo prazo\") é bom para o agente deste problema. Os resultados também indicam que a função de reforço padrão e a função de reforço reward_linear com uma pequena penalização para momentos nos quais a cobra não melhorou sua pontuação são adequadas para o problema.\n",
    "\n",
    "Para o método Monte Carlo, exploramos as funções de recompensa padrão e reward_euclidean, que busca penalizar a cobra por estar distante da maçã. Exploramos tanto a variante first-visit quanto a variante every-visit. Os bons casos que identificamos são:\n",
    "\n",
    "- (First Visit) N0 = 1, gamma = 0.2, reward_euclidean: pontuação máxima = 27, média das últimas 50 pontuações = 7.48, tempo de execução = 99.6431 segundos.\n",
    "- (First Visit) N0 = 1, gamma = 0.2, default_reward: pontuação máxima = 28, média das últimas 50 pontuações = 8.44, tempo de execução = 10.0414 segundos.\n",
    "- (Every Visit) N0 = 1, gamma = 0.5, default_reward: pontuação máxima = 50, média das últimas 50 pontuações = 2.2020, tempo de execução = 10.5174 segundos.\n",
    "- (Every Visit) N0 = 1, gamma = 1, default_reward: pontuação máxima = 49, média das últimas 50 pontuações = 16.84, tempo de execução = 8.6560 segundos.\n",
    "\n",
    "Monte Carlo every-visit foi melhor que first-visit para o problema, mas ainda bastante inferior aos métodos SARSA e Q-Learning em termos de pontuação máxima. Como o método de Monte Carlo atualiza os valores da função ação-valor somente ao final de um episódio (jogo), esse resultado é compatível com nossa expectativa de que seu desempenho fosse pior para este problema. Todos os métodos foram comparados com a mesma quantidade de episódios (1000), e acreditamos que o método de Monte Carlo precisaria de mais episódios para atingir pontuações maiores.\n",
    "\n",
    "Quanto à função de recompensa reward_euclidean, observamos que, para o início do jogo, funciona bem, incentivando a cobra a se aproximar da maçã. No entanto, conforme o comprimento da cobra aumenta, se aproximar da maçã sem antes se afastar para contornar e evitar colisões com seu próprio corpo causa sua morte. Assim, reward_euclidean é boa para os estágios iniciais do jogo, mas não para todo o jogo. Um experimento futuro interessante seria alterar a função de reforço conforme o comprimento da cobra, unindo as melhores características da função padrão à função reward_euclidean.\n",
    "\n",
    "# Observações\n",
    "\n",
    "Na próxima seção, colocamos o código-fonte da implementação. Ao executá-lo, uma janela com o jogo será aberta, mas ficará preta pois desabilitamos os recursos gráficos a fim de realizar medições mais rápidas dos dados que precisamos.\n",
    "\n",
    "Se desejar ver as animações dos experimentos, apesar de demorarem, basta alterar o parâmetro display das chamadas a run_q_learning, run_monte_carlo e run_sarsa para True.\n",
    "\n",
    "# Referências\n",
    "\n",
    "- Aulas da professora, disponibilizadas no Google Classroom da disciplina.\n",
    "\n",
    "- Livro Reinforcement Learning: An Introduction, Sutton and Barto, 2a Edição.\n",
    "\n",
    "### Artigos e exemplos de implementação\n",
    "\n",
    "- Artigo sobre Snake com Deep Q-Learning: https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n",
    "\n",
    "- Repositório do GitHub com Snake e Deep Q-Learning, no qual nos baseamos inicialmente para realizar o projeto: https://github.com/maurock/snake-ga\n",
    "\n",
    "- Exemplo de implementação do SARSA: https://www.geeksforgeeks.org/expected-sarsa-in-reinforcement-learning/\n",
    "\n",
    "- Artigo sobre SARSA(lambda) para vários jogos, inclusive Snake: http://cs229.stanford.edu/proj2012/JohnsonRobertsFisher-LearningToPlay2DVideoGames.pdf\n",
    "\n",
    "- Exemplo de implementação de Aproximador de Função: https://github.com/metastableB/Naagin-Naggin/blob/master/dlsnake/agents/approxQAgent.py\n",
    "\n",
    "- Exemplos de implementações de SARSA e Monte Carlo: https://github.com/ralhadeff/machine-learning-tools/tree/master/ReinforcementLearning\n",
    "\n",
    "- Exemplos de implementações baseados no livro Reinforcement Learning: An Introduction: https://github.com/flyywh/reinforcement-learning-1\n",
    "\n",
    "- Artigos sobre Snake, Q-Learning e SARSA: https://dkdennis.xyz/static/Nagging-report.pdf, http://cs229.stanford.edu/proj2016spr/report/060.pdf\n",
    "\n",
    "### Outras páginas\n",
    "\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
    "\n",
    "- https://towardsdatascience.com/slitherin-solving-the-classic-game-of-snake-with-ai-part-2-general-purpose-random-monte-25dc0dd4c4cf\n",
    "\n",
    "- https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566\n",
    "\n",
    "- https://towardsdatascience.com/monte-carlo-learning-b83f75233f92\n",
    "\n",
    "- https://medium.com/reinforcement-learning-a-step-by-step-implementati/reinforcement-learning-a-step-by-step-implementation-using-sarsa-1cfd3e64775a\n",
    "\n",
    "- https://www.programmersought.com/article/94202345056/\n",
    "\n",
    "- https://www.youtube.com/watch?v=l0sFUU7vScA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Código-fonte e experimentos\n",
    "\n",
    "Nessa seção, juntamos o código-fonte e os experimentos desenvolvidos para cada algoritmo, que originalmente foram mantidos em Jupyter Notebooks separados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimenos com Q-Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, QLearningAgent\n",
    "from environment import Environment\n",
    "from screen import Screen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_q_learning(agent: Agent, reward_function, episodes, display, speed, verbose=True):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "                    \n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            decoded_action1 = decode_action(action1)\n",
    "            decoded_action2 = decode_action(action2)\n",
    "            agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "            \n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "        \n",
    "\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=200, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.95\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.75\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.25\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 2\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 4\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 10\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experimentando variar a função de recompensa\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reward1(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -1 otherwise\n",
    "    \"\"\"\n",
    "    reward = -1\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "\n",
    "    return reward\n",
    "    \n",
    "\n",
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=reward1, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_exp(env, max_value, max_dist=3, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        +max_value * exp(dist[snake, food]) if dist[snake, food] <= max_dist\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    \n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = dist_metric(player_pos, food_pos)\n",
    "    \n",
    "    reward = max_value * math.exp(dist)\n",
    "    if env.game.crash:\n",
    "        reward = -1000\n",
    "    elif env.player.eaten:\n",
    "        reward = 1000\n",
    "        non_improvement = 0\n",
    "    elif dist >= max_dist:\n",
    "        reward = -non_improvement*0.01\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_exp(env, max_value=10),\n",
    "                         episodes=1000, speed=1, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_linear(env, penalty_rate=0.01, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    reward = -non_improvement*penalty_rate\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "        non_improvement = 0\n",
    "    else:\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=reward_linear,\n",
    "                         episodes=1000, speed=1, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.02),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.05),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.1),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.2),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.98\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "qLearningAgent = QLearningAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_q_learning(qLearningAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.5),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com SARSA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, SARSAAgent\n",
    "from environment import Environment\n",
    "from screen import Screen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_sarsa(agent: Agent, reward_function, episodes, display, speed, verbose=True):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "                    \n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            decoded_action1 = decode_action(action1)\n",
    "            decoded_action2 = decode_action(action2)\n",
    "            agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "            \n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "        \n",
    "\n",
    "    return metrics\n",
    "\n",
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.98\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.95\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.75\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.5\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.25\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 2\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experimento 3: Variando a função de recompensa\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reward1(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -1 otherwise\n",
    "    \"\"\"\n",
    "    reward = -1\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "\n",
    "    return reward\n",
    "    \n",
    "\n",
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=reward1, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_exp(env, max_value, max_dist=3, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        +max_value * exp(dist[snake, food]) if dist[snake, food] <= max_dist\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    \n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = dist_metric(player_pos, player_pos)\n",
    "    \n",
    "    reward = max_value * math.exp(dist)\n",
    "    if env.game.crash:\n",
    "        reward = -1000\n",
    "    elif env.player.eaten:\n",
    "        reward = 1000\n",
    "        non_improvement = 0\n",
    "    elif dist >= max_dist:\n",
    "        reward = -non_improvement*0.01\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_exp(env, max_value=10),\n",
    "                         episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def reward_linear(env, penalty_rate=0.01, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    reward = -non_improvement*penalty_rate\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "        non_improvement = 0\n",
    "    else:\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=reward_linear,\n",
    "                         episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.02),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.05),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.1),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.2),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1.0\n",
    "gamma = 0.99\n",
    "global non_improvement \n",
    "non_improvement = 0\n",
    "\n",
    "# define agent\n",
    "sarsaAgent = SARSAAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_sarsa(sarsaAgent, reward_function=lambda env: reward_linear(env, penalty_rate=0.5),\n",
    "                             episodes=1000, speed=0, display=False, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com Monte Carlo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importações e funções auxiliares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, QLearningAgent, MonteCarloAgent\n",
    "from environment import Environment\n",
    "from screen import Screen\n",
    "\n",
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11\n",
    "\n",
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()\n",
    "\n",
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n",
    "\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monte Carlo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Every Visit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_monte_carlo(agent: Agent, reward_function, episodes, display, speed, verbose=True, first_visit=False):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "\n",
    "    returns_sum = {} #defaultdict(float)\n",
    "    returns_count = {} #defaultdict(float)\n",
    "\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        states_and_actions_visited = []\n",
    "\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "\n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            #decoded_action1 = decode_action(action1)\n",
    "            #decoded_action2 = decode_action(action2)\n",
    "            #agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "\n",
    "            states_and_actions_visited.append((state2, action2, reward))\n",
    "\n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        # Acabou o episódio, hora de aprender\n",
    "        #sa_in_episode = set([(tuple(x[0]), x[1]) for x in states_and_actions_visited])\n",
    "        G = 0\n",
    "        for i, step in enumerate(states_and_actions_visited[::-1]):\n",
    "            state, action, r = step\n",
    "            G = agent.gamma*G + r\n",
    "\n",
    "            do_update = True\n",
    "\n",
    "            if first_visit:\n",
    "                if state in [x[0] for x in states_and_actions_visited[::-1][len(states_and_actions_visited)-i:]]:\n",
    "                    do_update = False\n",
    "\n",
    "            if do_update:\n",
    "                sa_pair = (step[0], decode_action(step[1]))\n",
    "\n",
    "                if sa_pair in returns_sum:\n",
    "                    returns_sum[sa_pair] += G\n",
    "                else:\n",
    "                    returns_sum[sa_pair] = G\n",
    "\n",
    "                if sa_pair in returns_count:\n",
    "                    returns_count[sa_pair] += 1.0\n",
    "                else:\n",
    "                    returns_count[sa_pair] = 1\n",
    "\n",
    "                average = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "                agent.Q[sa_pair[0], sa_pair[1] ] = average\n",
    "                #agent.Q[state, action] = agent.Q[state, action] + 1/returns_count[sa_pair]\n",
    "\n",
    "        # Incrementar episódios e ir guardando as nossas métricas\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "        \n",
    "        print('Run time:', (end-start), 'seconds')\n",
    "        print('Max. Score:', max(metrics['scores']))\n",
    "        print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))\n",
    "\n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.5\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.2\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First Visit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.5\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.2\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=default_reward, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experimentando reward_euclidean no Monte Carlo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importações e funções auxiliares"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# our classes\n",
    "from agent import Agent, QLearningAgent, MonteCarloAgent\n",
    "from environment import Environment\n",
    "from screen import Screen\n",
    "\n",
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11\n",
    "\n",
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()\n",
    "\n",
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n",
    "\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def reward_euclidean(env):\n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = distance.euclidean(player_pos, food_pos)\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10000\n",
    "    elif env.player.eaten:\n",
    "        reward = 10000\n",
    "    else:\n",
    "        if dist > 100:\n",
    "            reward = (-1*dist)/10\n",
    "        if dist <= 100:\n",
    "            reward = (100-dist)/10\n",
    "\n",
    "    return reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Every Visit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_monte_carlo(agent: Agent, reward_function, episodes, display, speed, verbose=True, first_visit=False):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "\n",
    "    returns_sum = {} #defaultdict(float)\n",
    "    returns_count = {} #defaultdict(float)\n",
    "\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        states_and_actions_visited = []\n",
    "\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "\n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            #decoded_action1 = decode_action(action1)\n",
    "            #decoded_action2 = decode_action(action2)\n",
    "            #agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "\n",
    "            states_and_actions_visited.append((state2, action2, reward))\n",
    "\n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        # Acabou o episódio, hora de aprender\n",
    "        #sa_in_episode = set([(tuple(x[0]), x[1]) for x in states_and_actions_visited])\n",
    "        G = 0\n",
    "        for i, step in enumerate(states_and_actions_visited[::-1]):\n",
    "            state, action, r = step\n",
    "            G = agent.gamma*G + r\n",
    "\n",
    "            do_update = True\n",
    "\n",
    "            if first_visit:\n",
    "                if state in [x[0] for x in states_and_actions_visited[::-1][len(states_and_actions_visited)-i:]]:\n",
    "                    do_update = False\n",
    "\n",
    "            if do_update:\n",
    "                sa_pair = (step[0], decode_action(step[1]))\n",
    "\n",
    "                if sa_pair in returns_sum:\n",
    "                    returns_sum[sa_pair] += G\n",
    "                else:\n",
    "                    returns_sum[sa_pair] = G\n",
    "\n",
    "                if sa_pair in returns_count:\n",
    "                    returns_count[sa_pair] += 1.0\n",
    "                else:\n",
    "                    returns_count[sa_pair] = 1\n",
    "\n",
    "                average = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "                agent.Q[sa_pair[0], sa_pair[1] ] = average\n",
    "                #agent.Q[state, action] = agent.Q[state, action] + 1/returns_count[sa_pair]\n",
    "\n",
    "        # Incrementar episódios e ir guardando as nossas métricas\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "\n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        print()\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "N0 = 1\n",
    "gamma = 0.2\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First Visit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.2\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.5\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False, first_visit=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 0.5\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgent(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=1000, speed=0, display=False)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com SARSA(lambda)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com Q-Learning e Aproximador Linear de Função"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com SARSA e Aproximador Linear de Função"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com Monte Carlo e Aproximador Linear de Função"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimentos com SARSA(lambda) e Aproximador Linear de Função"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
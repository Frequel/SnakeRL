{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objetivos do Trabalho\n",
    "O objetivo deste trabalho é aplicar métodos de aprendizado por reforço ao problema Snake, sendo estes métodos um on-policy e um off-policy, a fim de avaliá-los quanto à qualidade das soluções encontradas e custo computacional. Também pretende-se comparar os resultados obtidos com estes novos métodos aos resultados obtidos previamente no Projeto 1.\n",
    "\n",
    "Para o projeto 2, mantivemos o mesmo problema do projeto 1 (o jogo Snake) e a mesma formulação MDP. Assim, a natureza do ambiente e os estados terminais são os mesmos do projeto 1. Também mantivemos as mesmas funções de reforço experimentadas no projeto 1. Dessa forma, o que muda em relação ao projeto 1 são os algoritmos empregados para resolver o problema. Realizamos experimentos com os métodos DQN (Deep Q-Learning) e A2C (Advantage Actor-Critic).\n",
    "\n",
    "# 2. Membros do Grupo e Contribuições\n",
    "\n",
    "### Ana Clara Zoppi Serpa (RA 165880)\n",
    "- Experimentos com DQN\n",
    "- Análise de resultados experimentais\n",
    "- Escrita do relatório\n",
    "- Gravação do vídeo\n",
    "\n",
    "### Gabriel Oliveira dos Santos (RA 197460)\n",
    "- Experimentos com A2C\n",
    "- Análise de resultados experimentais\n",
    "- Escrita do relatório\n",
    "- Adaptação do nosso embiente para usar a biblioteca Stable Baselines\n",
    "\n",
    "### Silvio Bento Garcia Junior (RA 265194)\n",
    "- Tentativas com PPO\n",
    "- Escrita do relatório\n",
    "- Análise de resultados experimentais\n",
    "- Revisão de relatório\n",
    "\n",
    "### Tito Barbosa Rezende (RA 025327)\n",
    "- Experimentos com DQN\n",
    "- Análise de resultados experimentais\n",
    "- Revisão de relatório\n",
    "\n",
    "### Matteo Di Fabio (RA 264339)\n",
    "- Experimentos com A2C\n",
    "- Adaptação do nosso embiente para usar a biblioteca Stable Baselines\n",
    "- Análise de resultados experimentais\n",
    "- Revisão de relatório\n",
    "\n",
    "# 3. Link para o vídeo\n",
    "\n",
    "https://drive.google.com/file/d/1A2MjxgTH53KoNBUeaakoid5Q8jydyJEh/view?usp=sharing\n",
    "\n",
    "# 4. Problema\n",
    "Escolhemos trabalhar com o jogo Snake, também conhecido como \"Serpente\" ou \"Jogo da Cobrinha\", disponível em https://www.google.com/fbx?fbx=snake_arcade.\n",
    "\n",
    "O jogador é uma cobra, inicialmente pequena, e seu objetivo é comer maçãs. A cobra pode se mover para cima, para baixo, para a esquerda e para a direita. Maçãs aparecem em posições aleatórias do mapa, e o jogador deve se movimentar a fim de passar pela posição que contém a maçã atual. O jogador pode se mover livremente, mas não pode colidir com as bordas do mapa (paredes), nem com seu próprio corpo. Conforme as maçãs são consumidas, o tamanho da cobra aumenta, e há mais risco de colidir com seu próprio corpo ao se movimentar pelo mapa, assim aumentando a dificuldade do jogo.\n",
    "\n",
    "## 4.1 Formulação do problema como MDP\n",
    "\n",
    "Conforme visto em aula, um Processo Decisório de Markov (MDP) é uma tupla <S, P, A, R, gamma>, sendo S um conjunto finito de estados, A um conjunto finito de ações, P uma matriz de probabilidades de transições entre estados, R uma função de reforço e gamma um fator de desconto, que é um número real no intervalo [0,1].\n",
    "\n",
    "Um estado é uma tupla com 11 valores booleanos:\n",
    "1. Se há perigo (parede ou o próprio corpo da cobra) uma ou duas unidades do tabuleiro à frente;\n",
    "2. Se há perigo uma ou duas unidades à direita;\n",
    "3. Se há perigo uma ou duas unidades à esquerda;\n",
    "4. Se a cobra está se movendo para a esquerda;\n",
    "5. Se a cobra está se movendo para a direita;\n",
    "6. Se a cobra está se movendo para cima;\n",
    "7. Se a cobra está se movendo para baixo.\n",
    "8. Se há comida à esquerda;\n",
    "9. Se há comida à direita;\n",
    "10. Se há comida para cima;\n",
    "11. Se há comida para baixo;\n",
    "\n",
    "Há 2^11 estados possíveis.\n",
    "\n",
    "Essas direções (esquerda, direita, cima, baixo) são sempre consideradas na perspectiva da cabeça da cobra.\n",
    "\n",
    "As ações possíveis são:\n",
    "- Continuar indo na mesma direção, representada pelo vetor [1,0,0];\n",
    "- Virar para a direita, representada por [0,1,0];\n",
    "- Virar para a esquerda, representada por [0,0,1];\n",
    "\n",
    "A tela de jogo tem 440 pixels de largura e 440 pixels de altura. Cada parte do corpo da cobra é um quadrado de 20 pixels de altura e 20 pixels de largura. A maçã tem 20 pixels de altura e 20 pixels de largura também.\n",
    "\n",
    "A matriz de transição de probabilidades P contém somente uns e zeros, pois dada uma posição (estado) atual no tabuleiro e uma ação, o resultado é sempre o mesmo. Para cada estado atual **s**, estado resultante **s'** quando tomada uma ação **a** e estados restantes **Z** que não são obtidos tomando **a** a partir de **s**, a probabilidade de ir de **s** para **s'** é 1, enquanto que de **s** a qualquer dos **Z** é zero. Isso vale para todos os estados e ações, por isso a matriz terá somente zeros e uns.\n",
    "\n",
    "Quanto à função de reforço, definimos uma função padrão e depois exploramos outras. Isso será detalhado mais adiante no relatório, assim como os diferentes fatores de desconto (gamma) adotados nos experimentos.\n",
    "\n",
    "## 4.2 Natureza do ambiente\n",
    "\n",
    "* Determinístico ou estocástico: **determinístico**, pois, dado um estado corrente e uma ação, o estado resultante é sempre o mesmo (dada uma posição, a ação de continuar na mesma direção/virar para a direita/virar para a esquerda leva sempre ao mesmo destino).\n",
    "\n",
    "* Contínuo ou discreto: **discreto**, já que temos um conjunto finito de ações que podemos escolher tomar a partir de um certo estado (virar para a direita, virar para a esquerda, continuar na mesma direção).\n",
    "\n",
    "* Episódico ou não episódico: **episódico**, sendo cada episódio um jogo. O episódio termina somente quando a cobra morre.\n",
    "\n",
    "* Single-agent ou multi-agent: **single-agent**, pois nosso único agente é a cobra.\n",
    "\n",
    "## 4.3 Modelo de discretização adotado\n",
    "\n",
    "Como nosso ambiente já é discreto, não precisamos adotar nenhuma estratégia de discretização.\n",
    "\n",
    "## 4.4 Estados Terminais\n",
    "\n",
    "Nossos estados terminais são aqueles referentes à morte da cobra, portanto ocorrem somente auando a cobra colide com a parede e quando a cobra colide com seu próprio corpo.\n",
    "\n",
    "## 4.5 Funções de Reforço\n",
    "\n",
    "Nossa **função de reforço padrão** foi -50 quando a cobra morre, +10 quando come a maçã e 0 caso contrário.\n",
    "\n",
    "```\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -50 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "No projeto 1, experimentamos outras funções de reforço. No entanto, para os experimentos do projeto 2, decidimos manter sempre a `default_reward`, tendo em vista que apresentou bons resultados no projeto 1.\n",
    "\n",
    "# 5. Pontos-chave da implementação\n",
    "\n",
    "## 5.1 Implementação do Ambiente\n",
    "\n",
    "Neste projeto, nós precisamos utilizar o ``gym`` a fim de podermos fazer uso da ``stable-baselines``. Para se criar um ambiente customizado é necessário implementar a classe ``gym.Env``. Consequentemente, precimos implementar os métodos ``step(self, action)`` e ``reset(self)``, que são chamados a cada iteração e ao término de cada episódio, respectivamente. Outro método que é necessário para renderizar o jogo, é o ``render``. Além disso, também foi necessário definirmos os espaços de ação e observacional. Abaixo temos a estrutura geral do ambiente que nós criamos:\n",
    "\n",
    "```\n",
    "class SnakeEnv(gym.Env):\n",
    "    def __init__(self, game_width, game_height, reward_function=default_reward, enable_render=True):\n",
    "        self.game_width = game_width\n",
    "        self.game_height = game_height\n",
    "        self.game = Game(game_width, game_height)\n",
    "        self.player = self.game.player\n",
    "        self.food = self.game.food\n",
    "        self.get_reward = reward_function\n",
    "        self.enable_render = enable_render\n",
    "        self.screen = Screen(self.game, self.player, self.food)\n",
    "        num_state = 2 ** 11\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Discrete(num_state))\n",
    "\n",
    "    def step(self, action):        \n",
    "        self.player.do_move(action)\n",
    "        state = self.__get_state()\n",
    "        reward = self.get_reward(self)\n",
    "        done = self.game.crash\n",
    "        info = {}\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.game = Game(self.game_width, self.game_height)\n",
    "        self.player = self.game.player\n",
    "        self.food = self.game.food\n",
    "\n",
    "        self.screen.game = self.game\n",
    "        self.screen.player = self.player\n",
    "        self.screen.food = self.food\n",
    "        self.count = 0\n",
    "        return self.__get_state()\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        if self.enable_render:\n",
    "            self.screen.display()\n",
    "            pygame.time.wait(1)\n",
    "```\n",
    "   \n",
    "No nosso ambiente definimos o espaço de ação como ``action_space = spaces.Discrete(3)``, pois temos apenas 3 ações possíveis: seguir na mesma direção, virar para a esquerda, virar para direita. Já o espaço observacional é definido como ``observation_space = spaces.Discrete(2**11))``, dessa forma cada estado é representado por um número cuja representação binário tem 11 algarismos.\n",
    "\n",
    "## 5.2 Exemplo de execução de experimento\n",
    "\n",
    "A exucução de um experimento acontece pelo seguinte trecho de codigo:\n",
    "\n",
    "```\n",
    "for i in range(50):\n",
    "    # enable_render=True turns on the display\n",
    "    snake_env = SnakeEnv(440, 440, enable_render=False)\n",
    "    env = make_vec_env(lambda: snake_env, n_envs=1)\n",
    "\n",
    "    gamma = random.uniform(0.95, 1)\n",
    "    \n",
    "    model = A2C(MlpPolicy, env, verbose=1, learning_rate=1e-3, seed=42, gamma=gamma)\n",
    "    model.learn(total_timesteps=20000, log_interval=1000)\n",
    "\n",
    "    # Print rewards and scores for each episode\n",
    "    max_train_score = snake_env.record\n",
    "    mean_train_score = np.mean(np.array(snake_env.results['score']))\n",
    "    max_test_score, mean_test_score = evaluate(model)\n",
    "    print(\"iteration \", i, \": gamma\", gamma)\n",
    "    results[gamma] = (snake_env.results, max_train_score, max_test_score, mean_train_score, mean_test_score)\n",
    "```\n",
    " \n",
    "Neste trecho de código definimos alguns parametros que serão variados a fim de medir o desempenho dos métodos testados, para alterar o método basta trocar \"A2C\" em `model = A2C(MlpPolicy, env, verbose=1, learning_rate=1e-3, seed=42, gamma=gamma)` por, por exemplo, \"DQN\". A biblioteca utilizada já tem os metodos ultilizados implementados, sendo assim basta trocar A2C e adequar os parâmetros para o experimento.\n",
    "\n",
    "O trecho seguinte à chamada do método `learn` imprime os resultados.\n",
    "\n",
    "## 5.3 Particularidades e restrições da implementação\n",
    "\n",
    "Para executar o código, é preciso instalar a biblioteca PyGame. Isso pode ser feito com o comando `pip install pygame` no terminal. Também é preciso instalar a biblioteca Stable Baselines 3. Isso pode ser feito com o comando `pip install stable-baselines3`.\n",
    "\n",
    "# 6. Métodos\n",
    "\n",
    "## 6.1 DQN (Deep Q-Learning)\n",
    "\n",
    "### 6.1.1 Ideia geral do método\n",
    "\n",
    "O aprendizado por reforço pode ser suficientemente aplicável ao ambiente onde todos os estados alcançáveis podem ser gerenciados e armazenados na memória RAM padrão do computador. No entanto, no ambiente em que o número de estados supera a capacidade dos computadores, a abordagem padrão de RL não é muito aplicável. Além disso, em ambiente real, o agente tem que enfrentar problemas de estados contínuos, variáveis contínuas e de controle contínuo.\n",
    "\n",
    "Tendo em mente a complexidade do ambiente em que o agente deve operar, a tabela Q de estados ações usada no RL é substituída por uma Rede Neural Profunda que mapeia o ambiente (aproximação não linear) para ações do agente. A arquitetura da rede, a escolha dos hiperparâmetros da rede e o aprendizado são realizados durante a fase de treinamento. O DQN permite que o agente explore ambientes não estruturados e adquira conhecimentos que, com o tempo, possibilitem a imitação do comportamento humano.\n",
    "\n",
    "Durante o processo de treinamento, o agente interage com o ambiente e recebe dados, que são utilizados durante o aprendizado da Q-network. O agente explora o ambiente para construir uma imagem completa das transições e resultados da ação. No início, o agente decide sobre as ações de forma aleatória. Enquanto explora o ambiente, o agente tenta olhar para a rede Q (aproximação) para decidir como agir.\n",
    "\n",
    "É utilizado um método epsilon-guloso. Para tornar o processo de treinamento mais estável, aplica-se um buffer de replay que memoriza experiências do comportamento do agente. Em seguida, o treinamento é realizado em amostras aleatórias do buffer.\n",
    "\n",
    "### 6.1.2 Pseudocódigo\n",
    "![](DQN.PNG)\n",
    "\n",
    "### 6.1.2 Passos do Algoritimo\n",
    "1. Inicializar o buffer de replay.\n",
    "\n",
    "2. Pré-processar o ambiente e passe o estado S como entrada para a rede neural, que retornará os valores Q para cada ação possível a partir do estado S.\n",
    "\n",
    "3. Selecionar uma ação com a política epsilon-greedy, ou seja: com probabilidade epsion, selecionamos uma ação aleatória A e, com probabilidade 1-epsilon, selecionamos uma ação A para a qual Q é máximo.\n",
    "\n",
    "4. Tendo escolhido A, o agente realiza essa ação a partir de S para chegar em S' e receber uma recompensa R.\n",
    "\n",
    "5. Guardar essa transição no buffer de replay (<S,A,R,S’>).\n",
    "\n",
    "6. Amostrar alguns batches aleatórios de transições do replay buffer e calcular a loss.\n",
    "\n",
    "7. Descida do gradiente para minimizar loss da rede neural.\n",
    "\n",
    "8. Após k passos, atualizar os pesos da rede alvo com os pesos da rede real.\n",
    "\n",
    "9. Realizar esses passos por M episódios. \n",
    "\n",
    "### 6.1.3 Parâmetros\n",
    "\n",
    "A biblioteca Stable Baselines (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) permite alterar diversos parâmetros do método DQN. Neste trabalho, exploramos:\n",
    "\n",
    "- **total_timesteps**: o número total de amostras, ou seja, de passos do ambiente (\"environment steps\") que serão usados para o treinamento.\n",
    "- **learning_starts**: a partir de qual passo o aprendizado deve começar. Em outras palavras, são coletadas learning_steps transições (passos) do modelo antes de começar de fato o treinamento.\n",
    "- **gamma**: fator de desconto para recompensas futuras. Este fator regula o tipo de atitude do agente em relação a recompensas de longo prazo - ele pode priorizar recompensas futuras (quanto mais próximo de 1 gamma estiver) ou priorizar recompensas imediatas (quanto mais próximo de 0 gamma estiver).\n",
    "- **exploration_fraction**: fração do período de treinamento na qual a taxa de exploração, i.e, escolha de ações aleatórias, é diminuída.\n",
    "- **learning_rate**: taxa de aprendizado.\n",
    "- **batch_size**: tamanho do batch para cada atualização de gradiente.\n",
    "\n",
    "\n",
    "## 6.2 A2C (Advantage Actor-Critic)\n",
    "\n",
    "### 6.2.1 Conceito do método\n",
    "\n",
    "O A2C é uma variante do A3C, ambos são Actor-Critics e buscam tirar proveito de todas as coisas boas tanto dos algoritmos baseados em valor quanto dos baseados em políticas, eliminando suas desvantagens. A ideia principal é dividir o modelo em duas partes: uma para calcular uma ação com base em um estado e outra para produzir os valores Q da ação. Para a primeira parte o ator toma como entrada o estado e produz a melhor ação, ele controla como o agente se comporta, aprendendo a política ótima. Para a segunda parte avalia-se a ação computando a função de valor. \n",
    "\n",
    "Nssas combinação de dois modelos, as partes participam de um jogo em que ambas melhoram em seus papéis com o passar do tempo. O resultado é que a arquitetura geral aprenderá a jogar o jogo com mais eficiência do que os dois métodos separadamente.\n",
    "\n",
    "O ator pode ser um aproximador de função como uma rede neural, sua tarefa é produzir a melhor ação para um determinado estado. O crítico é outro aproximador de função, que recebe como entrada o ambiente e a ação do ator, concatena-os e produz o valor da ação (valor Q) para o par dado. O treinamento das duas redes é realizado separadamente e utiliza gradiente ascendente para atualizar seus pesos. Com o passar do tempo, o ator vai aprendendo a produzir ações cada vez melhores e o crítico vai ficando cada vez melhor na avaliação dessas ações.\n",
    "\n",
    "Os pontos apresentados anteriormente definem um algoritimo Actor-Critic, o nosso aloritimo faz parte desta familia Advantage Actor-Critic (A2C), onde Advantage vem do conceito onde os valores Q podem, de fato, ser decompostos em duas partes: a função de valor de estado V(s) e o valor de vantagem A(s, a):\n",
    "\n",
    "Q(s,a)=V(s)+A(s,a) => A(s,a) = Q(s,a)-V(s)=> A(s,a) = r+V(s_hat)-(s)\n",
    "\n",
    "A função de vantagem captura o quão melhor uma ação é comparada às outras em um determinado estado, enquanto a função de valor captura o quão bom é estar neste estado. \n",
    "\n",
    "Em vez de fazer com que o crítico aprenda os valores Q, fazemos com que ele aprenda os valores da vantagem; desta forma, a avaliação de uma ação é baseada não apenas em quão boa é a ação, mas também em quão melhor ela pode ser em relação a outras.\n",
    "\n",
    "### 6.2.2 Pseudocódigo\n",
    "\n",
    "![](A2C.PNG)\n",
    "\n",
    "### 6.2.3 Parâmetros\n",
    "\n",
    "A biblioteca Stable Baselines (https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) permite alterar diversos parâmetros do método A2C. Neste trabalho, exploramos:\n",
    "\n",
    "- **total_timesteps**: o número total de amostras, ou seja, de passos do ambiente (\"environment steps\") que serão usados para o treinamento.\n",
    "- **gamma**: fator de desconto para recompensas futuras. Este fator regula o tipo de atitude do agente em relação a recompensas de longo prazo - ele pode priorizar recompensas futuras (quanto mais próximo de 1 gamma estiver) ou priorizar recompensas imediatas (quanto mais próximo de 0 gamma estiver).\n",
    "- **learning_rate**: taxa de aprendizado.\n",
    "- **normalize_advantage**: normalizar (true) ou não (false) a vantagem.\n",
    "- **ent_coef**: coeficiente de entropia para o cálculo da loss.\n",
    "- **vf_coef**: coeficiente da função valor para o cálculo da loss.\n",
    "- **use_rms_prop**: utilizar o otimizador RMSprop (true) ou utilizar o otimizador Adam (false).\n",
    "- **n_eval_episode**: número de episódios para avaliar o agente.\n",
    "\n",
    "## 6.3 Métricas\n",
    "Para avaliar a qualidade das soluções, coletamos, a cada experimento: Pontuação Máxima durante treinamento (Max Train Score), Pontuação Máxima durante teste (Max Test Score), Pontuação Média durante treinamento (Mean Train Score), Pontuação Média durante teste (Mean Test Score).\n",
    "\n",
    "# 7. Discussão de Principais Resultados\n",
    "\n",
    "## 7.1 Método DQN e seus parâmetros\n",
    "\n",
    "### 7.1.1 Diferentes valores para total_timesteps e learning_starts, gammas aleatórios\n",
    "\n",
    "Para este conjunto de experimentos, foram variados total_timesteps e learning_starts, a fim de explorar o impacto da diferença entre a quantidade total de passos e o ponto a partir do qual o aprendizado se inicia. Para cada par (total_timesteps, learning_starts), foram escolhidos 10 valores de gamma aleatoriamente no intervalo (0.90, 1), a fim de realizar uma busca por bons parâmetros. Na tabela abaixo, resumimos os resultados: cada linha é o melhor resultado obtido com o par (total_timesteps, learning_starts), dentre os 10 obtidos.\n",
    "\n",
    "Observa-se que diminuir learning_starts melhora os resultados, exceto no caso no qual reduzimos de 1000 para 500 com total_timesteps = 20000. No entanto, como o gamma escolhido também variou, a piora pode se dever a uma má escolha de gamma. Também observa-se, como esperado, que uma maior quantidade de total_timesteps pode melhorar os resultados.\n",
    "\n",
    "| total_timesteps | learning_starts | gamma | learning_rate | batch_size | max train score | max test score | mean train score | mean test score |\n",
    "|:---------------:|:---------------:|:-----:|:-------------:|:----------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|      80000      |      50000      |  0.95 |    1,00E-03   |     32     |        9        |       11       |       0.63       |       2.14      |\n",
    "|      20000      |      10000      |  0.91 |    1,00E-03   |     32     |        6        |       14       |       0.63       |       2.29      |\n",
    "|      20000      |       5000      |  1.00 |    1,00E-03   |     32     |        12       |       19       |       1.30       |       3.49      |\n",
    "|      20000      |       1000      |  0.93 |    1,00E-03   |     32     |        10       |       17       |       1.36       |       3.31      |\n",
    "|      20000      |       500       |  0.91 |    1,00E-03   |     32     |        10       |       13       |       1.41       |       2.80      |\n",
    "|      40000      |       1000      |  0.93 |    1,00E-03   |     32     |        18       |       23       |       2.15       |       4.90      |\n",
    "|      80000      |       1000      |  0.99 |    1,00E-03   |     32     |        27       |       23       |       2.32       |       5.08      |\n",
    "\n",
    "### 7.1.2 Diferentes valores para batch_size, gammas aleatórios\n",
    "\n",
    "Neste experimento, variamos batch_size e continuamos escolhendo 10 valores aleatórios de gamma, a fim de encontrar uma boa configuração. A melhor configuração encontrada foi com batch_size=128, resultando em Max Test Score = 37. Essa foi a melhor configuração dentre todos os experimentos realizados com total_timesteps menor que 200000 para o método DQN, tanto considerando experimentos de gammas fixos como experimentos de busca aleatória por bons parâmetros.  \n",
    "\n",
    "| total_timesteps | learning_starts | gamma | learning_rate | batch_size | max train score | max test score | mean train score | mean test score |\n",
    "|:---------------:|:---------------:|:-----:|:-------------:|:----------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|      20000      |       1000      |  0.93 |    1,00E-03   |     32     |        10       |       17       |       1.36       |       3.31      |\n",
    "|      20000      |       1000      |  0.93 |    1,00E-03   |     64     |        9        |       15       |       1.48       |       2.60      |\n",
    "|      20000      |       1000      |  0.99 |    1,00E-03   |     128    |        11       |       37       |       1.37       |       5.78      |\n",
    "|      20000      |       1000      |  0.97 |    1,00E-03   |     256    |        18       |       26       |       1.90       |       3.04      |\n",
    "\n",
    "### 7.1.3 Teste 1 com 200000 timesteps\n",
    "\n",
    "Como learning_starts=1000, learning_rate=1,00E-03, batch_size=128 e gamma=0.9965353590176854 (arredondado para 0.99 na tabela) foi a melhor configuração, realizamos um teste com 200000 timesteps utilizando-a. No entanto, a melhoria não foi significativa - tínhamos obtido 37 pontos e, com 200000 timesteps, obtivemos 38 pontos máximos.\n",
    "\n",
    "| total_timesteps | learning_starts |        gamma       | learning_rate | batch_size | max train score | max test score | mean train score | mean test score |\n",
    "|:---------------:|:---------------:|:------------------:|:-------------:|:----------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|      200000     |       1000      | 0.9965353590176854 |    1,00E-03   |     128    |        39       |       38       |       3.64       |       7.90      |\n",
    "\n",
    "### 7.1.4 Variando exploration_fraction\n",
    "\n",
    "Para este conjunto de experimentos, mantivemos total_timesteps = 80000, learning_starts = 1, gamma = 0.99 e learning_rate = 0.001, variando somente o exploration_fraction de 0 a 0.9. O melhor Max Test Score foi 32, com exploration_fraction = 0.5.\n",
    "\n",
    "No entanto, não é possível dizer que há uma relação entre exploration_fraction e a pontuação máxima - em alguns casos, um exploration_fraction menor obteve resultados melhores que um exploration_fraction maior, e vice-versa.\n",
    "\n",
    "![](DQN_exploration_fraction.png)\n",
    "\n",
    "| exploration_fraction | learning_rate | Max Train Score | Max Test Score | Mean Train Score | Mean Test Score |\n",
    "|:--------------------:|:-------------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|          0.0         |     0.001     |        30       |        6       |       3.00       |       1.50      |\n",
    "|          0.1         |     0.001     |        27       |       26       |       2.38       |       4.87      |\n",
    "|          0.2         |     0.001     |        23       |       15       |       1.65       |       2.83      |\n",
    "|          0.3         |     0.001     |        14       |        7       |       0.88       |       2.27      |\n",
    "|          0.4         |     0.001     |        16       |        7       |       1.67       |       1.36      |\n",
    "|          **0.5**         |     0.001     |        25       |       **32**       |       2.15       |       5.59      |\n",
    "|          0.6         |     0.001     |        23       |       21       |       1.42       |       5.20      |\n",
    "|          0.7         |     0.001     |        20       |       16       |       1.06       |       3.20      |\n",
    "|          0.8         |     0.001     |        11       |       16       |       1.01       |       3.53      |\n",
    "|          0.9         |     0.001     |        20       |       17       |       1.32       |       3.20      |\n",
    "\n",
    "\n",
    "### 7.1.5 Variando learning_rate\n",
    "\n",
    "Para estes experimentos, mantivemos total_timesteps = 80000, learning_starts = 1, gamma = 0.99 e exploration_fraction = 0.1, variando somente learning_rate. O melhor Max Test Score foi 35, com learning_rate = 0.00010. Percebe-se que diminuir a learning_rate de 1 até 0.00010 resultou em Max Test Score maiores, mas, ao configurar learning_rate = 0.00001, obtivemos Max Test Score = 3. Isso sugere que o intervalo de 0.00010 a 0.01 é uma boa faixa para a learning_rate, sendo 1 um valor muito alto e 0.00001 um valor muito baixo.\n",
    "\n",
    "![](DQN_learning_rate.png)\n",
    "\n",
    "| exploration_fraction | learning_rate | Max Train Score | Max Test Score | Mean Train Score | Mean Test Score |\n",
    "|:--------------------:|:-------------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|          0.1         |     1.0000    |        2        |      1.00      |       0.18       |       0.07      |\n",
    "|          0.1         |    0.10000    |        3        |      1.00      |       0.14       |       0.01      |\n",
    "|          0.1         |    0.01000    |        2        |      14.00     |       1.94       |       3.07      |\n",
    "|          0.1         |    0.00100    |        2        |      26.00     |       2.38       |       4.87      |\n",
    "|          0.1         |    0.00010    |        4        |      35.00     |       5.14       |       7.96      |\n",
    "|          0.1         |    0.00001    |        3        |      3.00      |       0.14       |       0.13      |\n",
    "\n",
    "### 7.1.6 Teste 2 com 200000 timesteps\n",
    "\n",
    "Um teste final com total_timesteps=200000, learning_rate=1, gamma=0.99, exploration_fraction=0.2 e learning_rate=0.00010 foi realizado a fim de avaliar se, com mais tempo de treinamento, é possível obter resultados melhores. Obtivemos Max Test Score = 32.\n",
    "\n",
    "| exploration_fraction | learning_rate | Max Train Score | Max Test Score | Mean Train Score | Mean Test Score |\n",
    "|:--------------------:|:-------------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|          0.2         |    0.00010    |        43       |       32       |       5.37       |       7.98      |\n",
    "\n",
    "\n",
    "## 7.2 Método A2C e seus parâmetros\n",
    "\n",
    "### 7.2.1 Exploração aleatória de gamma\n",
    "\n",
    "Mantendo total_timesteps=2000, learning_rate=0.001 e os demais parâmetros do método A2C com seus valores padrão, escolhemos 50 valores aleatórios para gamma a fim de descobrir bons valores. O gráfico abaixo ilustra os resultados obtidos.\n",
    "\n",
    "![](A2C_gammas.png)\n",
    "\n",
    "Não apresentamos a tabela no relatório pois há muitos valores, mas é possível consultar os notebooks auxiliares e as planilhas de experimentos para verificar (ver seção \"Observações\"). O melhor resultado encontrado com essa estratégia foi Max Test Score = 39.\n",
    "\n",
    "### 7.2.2 Configurando normalize_advantage = true\n",
    "\n",
    "Por padrão, no método A2C a flag normalize_advantage é configurada para false. Experimentamos trocá-la para true, mas os resultados pioraram significativamente em relação aos resultados com configuração padrão, conforme a tabela abaixo. Nas primeiras duas linhas, temos o pior resultado com normalize_advantage = false (Max Test Score = 16) e o melhor (Max Test Score = 39). Com normalize_advantage = true, todos os casos foram piores que o pior resultado da configuração anterior - isso sugere que usar normalize_advantage = true não é uma boa estratégia para o problema.\n",
    "\n",
    "| total_timesteps | gamma | learning_rate | normalize_advantage | ent_coef | vf_coef | max train score | max test score | mean train score | mean test score |\n",
    "|:---------------:|:-----:|:-------------:|:-------------------:|:--------:|:-------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|      20000      |  0.99 |    1.00E-03   |        false        |    0.0   |   0.5   |        27       |       16       |       3.20       |       3.46      |\n",
    "|      20000      |  0.98 |    1.00E-03   |        false        |    0.0   |   0.5   |        24       |       39       |       3.21       |       7.71      |\n",
    "|      20000      |  0.96 |    1.00E-03   |         true        |    0.0   |   0.5   |        10       |       11       |       1.53       |       2.40      |\n",
    "|      20000      |  0.98 |    1.00E-03   |         true        |    0.0   |   0.5   |        3        |        0       |       0.42       |       0.00      |\n",
    "|      20000      |  0.98 |    1.00E-03   |         true        |    0.0   |   0.5   |        4        |        5       |       0.57       |       1.54      |\n",
    "|      20000      |  0.99 |    1.00E-03   |         true        |    0.0   |   0.5   |        3        |        2       |       0.73       |       0.86      |\n",
    "|      20000      |  0.99 |    1.00E-03   |         true        |    0.0   |   0.5   |        3        |        4       |       0.81       |       1.44      |\n",
    "|      20000      |  0.97 |    1.00E-03   |         true        |    0.0   |   0.5   |        3        |        2       |       0.26       |       0.47      |\n",
    "|      20000      |  0.99 |    1.00E-03   |         true        |    0.0   |   0.5   |        3        |        0       |       0.22       |       0.00      |\n",
    "|      20000      |  0.97 |    1.00E-03   |         true        |    0.0   |   0.5   |        17       |       14       |       1.48       |       2.38      |\n",
    "|      20000      |  0.99 |    1.00E-03   |         true        |    0.0   |   0.5   |        9        |       11       |       0.97       |       2.35      |\n",
    "|      20000      |  1.00 |    1.00E-03   |         true        |    0.0   |   0.5   |        13       |       18       |       1.96       |       3.75      |\n",
    "\n",
    "### 7.2.3 Variando enf_coef\n",
    "\n",
    "Para este experimento, gamma foi mantido fixo (0.98) e variamos o parâmetro ent_coef. No entanto, observamos que a qualidade das soluções (Max Test Score) não melhorou em relação à configuração inicial.\n",
    "\n",
    "![](A2C_entropy.png)\n",
    "\n",
    "| gamma | learning_rate | normalize_advantage | ent_coef | vf_coef | max train score | max test score | mean train score | mean test score |\n",
    "|:-----:|:-------------:|:-------------------:|:--------:|:-------:|:---------------:|:--------------:|:----------------:|:---------------:|\n",
    "|  0.98 |    1.00E-03   |        false        |   0.03   |   0.5   |        19       |       24       |       2.66       |       3.77      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.06   |   0.5   |        22       |       29       |       2.24       |       5.62      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.05   |   0.5   |        21       |       28       |       2.53       |       5.25      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.07   |   0.5   |        12       |       19       |       1.75       |       4.25      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.10   |   0.5   |        5        |        6       |       0.90       |       1.36      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.03   |   0.5   |        23       |       33       |       2.74       |       7.07      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.04   |   0.5   |        28       |       23       |       2.52       |       6.07      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.03   |   0.5   |        21       |       17       |       2.80       |       3.45      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.08   |   0.5   |        6        |        6       |       1.23       |       1.54      |\n",
    "|  0.98 |    1.00E-03   |        false        |   0.09   |   0.5   |        10       |       11       |       1.33       |       2.14      |\n",
    "\n",
    "### 7.2.4 Teste final com 200000 timesteps\n",
    "\n",
    "Escolhemos a melhor configuração dos experimentos anteriores e realizamos o treinamento com 200000 timesteps a fim de observar se, com mais tempo de treinamento, haveria melhoria dos resultados. Houve uma melhoria significativa: anteriormente, com apenas 20000 timesteps, obtivemos 39 pontos e, agora, com 200000, foram obtidos 55 pontos.\n",
    "\n",
    "|  gamma | learning_rate | normalize_advantage | ent_coef | vf_coef | max train score | max test score | mean train score | mean test score |       |\n",
    "|:------:|:-------------:|:-------------------:|:--------:|:-------:|:---------------:|:--------------:|:----------------:|:---------------:|-------|\n",
    "| 200000 |      0.98     |       1.00E-03      |   false  |   0.0   |       0.5       |       55       |        45        |       8.03      | 11.07 |\n",
    "\n",
    "## 7.3 Comparando DQN e A2C\n",
    "\n",
    "O melhor resultado conseguido com o método DQN foi Max Test Score = 39 (com 200000 timesteps). O método A2C, também com 200000 timesteps, obteve Max Test Score = 55. Assim, o A2C tem desempenho melhor para nosso problema.\n",
    "\n",
    "## 7.4 Comparação com resultados do projeto 1\n",
    "\n",
    "Na tabela abaixo, temos as melhores pontuações para cada algoritmo do projeto 1: Q-Learning, SARSA, SARSA-lambda, Monte Carlo First Visit, Monte Carlo Every Visit, Q-Learning com Aproximador Linear de Função (LFA), e assim por diante. Observa-se que:\n",
    "\n",
    "- Tanto Q-Learning quanto SARSA foram melhores que DQN e A2C;\n",
    "- SARSA-lambda foi melhor que DQN, mas não que A2C;\n",
    "- Monte Carlo Every Visit foi melhor que DQN, mas não que A2C;\n",
    "- DQN e AC2 superam todos os algoritmos do projeto 1 quando há utilização de Aproximador Linear de Função (LFA);\n",
    "\n",
    "Daí, é possível concluir que as Redes Neurais e estratégias de aprendizado aplicadas pelo DQN e pelo AC2 são melhores que aproximadores lineares de função para nosso problema, mas não chegam a ser melhores (pelo menos não com as configurações aqui testadas) que o método Q-Learning do projeto 1.\n",
    "\n",
    "| Algoritmo                   | Max Score  |\n",
    "|-----------------------------|------------|\n",
    "| Q-Learning                  |         72 |\n",
    "| SARSA                       |         70 |\n",
    "| SARSA-lambda                |         43 |\n",
    "| Monte Carlo First Visit     |         28 |\n",
    "| Monte Carlo Every Visit     |         50 |\n",
    "| Q-Learning LFA              |         23 |\n",
    "| SARSA LFA                   |         25 |\n",
    "| SARSA-lambda LFA            |         19 |\n",
    "| Monte Carlo First Visit LFA |         22 |\n",
    "| Monte Carlo Every Visit LFA |         26 |\n",
    "| **Melhor resultado com DQN**   |         39 |\n",
    "| **Melhor resultado com A2C**   |         55 |\n",
    "\n",
    "# 8. Observações\n",
    "\n",
    "## 8.1 Notebooks auxiliares, código completo e planilha de experimentos\n",
    "\n",
    "Aqui neste notebook, não colocamos as células de código dos nossos experimentos. No entanto, todos os nossos experimentos estão em notebooks auxiliares usados por cada membro do grupo.\n",
    "\n",
    "- Notebook auxiliar 1 - experimentos com DQN realizados por Ana Clara Zoppi Serpa: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/ana_DQN.ipynb\n",
    "- Notebook auxiliar 2 - experimentos com A2C realizados por Gabriel Oliveira dos Santos: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/gabriel.ipynb\n",
    "- Notebook auxiliar 3 - experimentos com DQN realizados por Tito Barbosa Rezende: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/tito_DQN.ipynb\n",
    "- Notebook auxiliar 4 - experimentos com A2C realizados por Matteo Di Fabio: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/Matteo.ipynb\n",
    "\n",
    "Além disso, todos os resultados estão disponíveis na seguinte planilha: https://docs.google.com/spreadsheets/d/1MHTG_k1Mc4TGba2h3Fi95k1qa0V8B3bf4FI8xGXIzMg/edit?usp=sharing\n",
    "\n",
    "O código completo está disponível no GitHub: https://github.com/AnaClaraZoppiSerpa/snake\n",
    "\n",
    "## 8.2 Outras\n",
    "\n",
    "Tentamos realizar experimentos com o método PPO (Proximal Policy Optimization), mas não obtivemos bons resultados. \n",
    "\n",
    "\n",
    "# Referências\n",
    "\n",
    "- Aulas da professora, disponibilizadas no Google Classroom da disciplina.\n",
    "\n",
    "- Livro Reinforcement Learning: An Introduction, Sutton and Barto, 2a Edição.\n",
    "\n",
    "- Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. PMLR, 2016.\n",
    "\n",
    "- Documentação da biblioteca Stable Baselines, método DQN: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "- Documentação da biblioteca Stable Baselines, método A2C: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html\n",
    "\n",
    "- Documentação da biblioteca Stable Baselines, criação de ambientes customizados: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\n",
    "\n",
    "- Explicação sobre Deep Q-Learning: https://en.m.wikipedia.org/wiki/Q-learning#Deep_Q-learning\n",
    "\n",
    "- Artigo sobre métodos actor-critic: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n",
    "- Mais sobre Deep Q-Learning: https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-introduction-deep-q-network-dqn-algorithm-fb74bf4d6862\n",
    "\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015). https://doi.org/10.1038/nature14236\n",
    "\n",
    "### Artigos e exemplos de implementação\n",
    "\n",
    "- Artigo sobre Snake com Deep Q-Learning: https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n",
    "\n",
    "- Repositório do GitHub com Snake e Deep Q-Learning, no qual nos baseamos inicialmente para realizar o projeto: https://github.com/maurock/snake-ga\n",
    "\n",
    "- Exemplo de implementação do SARSA: https://www.geeksforgeeks.org/expected-sarsa-in-reinforcement-learning/\n",
    "\n",
    "- Exemplo de implementação de Environment com Gym: https://github.com/pedrohbtp/snake-rl/blob/master/snake_rl.ipynb\n",
    "\n",
    "- Tutorial de Reinforcement Learning: https://araffin.github.io/slides/rl-tuto-jnrr19/#/4\n",
    "\n",
    "- Artigo sobre SARSA(lambda) para vários jogos, inclusive Snake: http://cs229.stanford.edu/proj2012/JohnsonRobertsFisher-LearningToPlay2DVideoGames.pdf\n",
    "\n",
    "- Exemplo de implementação de Aproximador de Função: https://github.com/metastableB/Naagin-Naggin/blob/master/dlsnake/agents/approxQAgent.py\n",
    "\n",
    "- Exemplos de implementações de SARSA e Monte Carlo: https://github.com/ralhadeff/machine-learning-tools/tree/master/ReinforcementLearning\n",
    "\n",
    "- Exemplos de implementações baseados no livro Reinforcement Learning: An Introduction: https://github.com/flyywh/reinforcement-learning-1\n",
    "\n",
    "- Artigos sobre Snake, Q-Learning e SARSA: https://dkdennis.xyz/static/Nagging-report.pdf, http://cs229.stanford.edu/proj2016spr/report/060.pdf\n",
    "\n",
    "### Outras páginas\n",
    "\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
    "\n",
    "- https://towardsdatascience.com/slitherin-solving-the-classic-game-of-snake-with-ai-part-2-general-purpose-random-monte-25dc0dd4c4cf\n",
    "\n",
    "- https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566\n",
    "\n",
    "- https://towardsdatascience.com/monte-carlo-learning-b83f75233f92\n",
    "\n",
    "- https://medium.com/reinforcement-learning-a-step-by-step-implementati/reinforcement-learning-a-step-by-step-implementation-using-sarsa-1cfd3e64775a\n",
    "\n",
    "- https://www.programmersought.com/article/94202345056/\n",
    "\n",
    "- https://www.youtube.com/watch?v=l0sFUU7vScA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

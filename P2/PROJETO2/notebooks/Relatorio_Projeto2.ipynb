{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coisas que falta escrever:\n",
    "\n",
    "characteristics of the selected algorithms\n",
    "\n",
    "parameters employed in the methods\n",
    "\n",
    "how the chosen methods work with the nature of the action-space representation\n",
    "\n",
    "on-policy x off-policy results\n",
    "\n",
    "did you achieve the policy?\n",
    "\n",
    "non-linear function approximation advantages and disadvantages in relation to project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2\n",
    "Para o projeto 2, mantivemos o mesmo problema do projeto 1 (o jogo Snake) e a mesma formulação MDP. Assim, a natureza do ambiente e os estados terminais são os mesmos do projeto 1. Também mantivemos as mesmas funções de reforço experimentadas no projeto 1. Dessa forma, o que muda em relação ao projeto 1 são os algoritmos empregados para resolver o problema.\n",
    "\n",
    "# Objetivos do Trabalho\n",
    "O objetivo deste trabalho é aplicar métodos de aprendizado por reforço ao problema Snake, sendo estes métodos um on-policy e um off-policy, a fim de avaliá-los quanto à qualidade das soluções encontradas e custo computacional. Também pretende-se comparar os resultados obtidos com estes novos métodos aos resultados obtidos previamente no Projeto 1.\n",
    "\n",
    "# Membros do Grupo e Contribuições\n",
    "\n",
    "### Ana Clara Zoppi Serpa (RA 165880)\n",
    "\n",
    "### Gabriel Oliveira dos Santos (RA 197460)\n",
    "\n",
    "### Silvio Bento Garcia Junior (RA 265194)\n",
    "\n",
    "### Tito Barbosa Rezende (RA 025327)\n",
    "\n",
    "### Matteo Di Fabio (RA 264339)\n",
    "\n",
    "## Link para o vídeo\n",
    "\n",
    "# Problema\n",
    "Escolhemos trabalhar com o jogo Snake, também conhecido como \"Serpente\" ou \"Jogo da Cobrinha\", disponível em https://www.google.com/fbx?fbx=snake_arcade.\n",
    "\n",
    "O jogador é uma cobra, inicialmente pequena, e seu objetivo é comer maçãs. A cobra pode se mover para cima, para baixo, para a esquerda e para a direita. Maçãs aparecem em posições aleatórias do mapa, e o jogador deve se movimentar a fim de passar pela posição que contém a maçã atual. O jogador pode se mover livremente, mas não pode colidir com as bordas do mapa (paredes), nem com seu próprio corpo. Conforme as maçãs são consumidas, o tamanho da cobra aumenta, e há mais risco de colidir com seu próprio corpo ao se movimentar pelo mapa, assim aumentando a dificuldade do jogo.\n",
    "\n",
    "## Formulação do problema como MDP\n",
    "\n",
    "Conforme visto em aula, um Processo Decisório de Markov (MDP) é uma tupla <S, P, A, R, gamma>, sendo S um conjunto finito de estados, A um conjunto finito de ações, P uma matriz de probabilidades de transições entre estados, R uma função de reforço e gamma um fator de desconto, que é um número real no intervalo [0,1].\n",
    "\n",
    "Um estado é uma tupla com 11 valores booleanos:\n",
    "1. Se há perigo (parede ou o próprio corpo da cobra) uma ou duas unidades do tabuleiro à frente;\n",
    "2. Se há perigo uma ou duas unidades à direita;\n",
    "3. Se há perigo uma ou duas unidades à esquerda;\n",
    "4. Se a cobra está se movendo para a esquerda;\n",
    "5. Se a cobra está se movendo para a direita;\n",
    "6. Se a cobra está se movendo para cima;\n",
    "7. Se a cobra está se movendo para baixo.\n",
    "8. Se há comida à esquerda;\n",
    "9. Se há comida à direita;\n",
    "10. Se há comida para cima;\n",
    "11. Se há comida para baixo;\n",
    "\n",
    "Há 2^11 estados possíveis.\n",
    "\n",
    "Essas direções (esquerda, direita, cima, baixo) são sempre consideradas na perspectiva da cabeça da cobra.\n",
    "\n",
    "As ações possíveis são:\n",
    "- Continuar indo na mesma direção, representada pelo vetor [1,0,0];\n",
    "- Virar para a direita, representada por [0,1,0];\n",
    "- Virar para a esquerda, representada por [0,0,1];\n",
    "\n",
    "A tela de jogo tem 440 pixels de largura e 440 pixels de altura. Cada parte do corpo da cobra é um quadrado de 20 pixels de altura e 20 pixels de largura. A maçã tem 20 pixels de altura e 20 pixels de largura também.\n",
    "\n",
    "A matriz de transição de probabilidades P contém somente uns e zeros, pois dada uma posição (estado) atual no tabuleiro e uma ação, o resultado é sempre o mesmo. Para cada estado atual **s**, estado resultante **s'** quando tomada uma ação **a** e estados restantes **Z** que não são obtidos tomando **a** a partir de **s**, a probabilidade de ir de **s** para **s'** é 1, enquanto que de **s** a qualquer dos **Z** é zero. Isso vale para todos os estados e ações, por isso a matriz terá somente zeros e uns.\n",
    "\n",
    "Quanto à função de reforço, definimos uma função padrão e depois exploramos outras. Isso será detalhado mais adiante no relatório, assim como os diferentes fatores de desconto (gamma) adotados nos experimentos.\n",
    "\n",
    "## Natureza do ambiente\n",
    "\n",
    "* Determinístico ou estocástico: **determinístico**, pois, dado um estado corrente e uma ação, o estado resultante é sempre o mesmo (dada uma posição, a ação de continuar na mesma direção/virar para a direita/virar para a esquerda leva sempre ao mesmo destino).\n",
    "\n",
    "* Contínuo ou discreto: **discreto**, já que temos um conjunto finito de ações que podemos escolher tomar a partir de um certo estado (virar para a direita, virar para a esquerda, continuar na mesma direção).\n",
    "\n",
    "* Episódico ou não episódico: **episódico**, sendo cada episódio um jogo. O episódio termina somente quando a cobra morre.\n",
    "\n",
    "* Single-agent ou multi-agent: **single-agent**, pois nosso único agente é a cobra.\n",
    "\n",
    "## Modelo de discretização adotado\n",
    "\n",
    "Como nosso ambiente já é discreto, não precisamos adotar nenhuma estratégia de discretização.\n",
    "\n",
    "## Estados Terminais\n",
    "\n",
    "Nossos estados terminais são aqueles referentes à morte da cobra, portanto ocorrem somente auando a cobra colide com a parede e quando a cobra colide com seu próprio corpo.\n",
    "\n",
    "## Funções de Reforço\n",
    "\n",
    "Nossa **função de reforço padrão** foi -50 quando a cobra morre, +10 quando come a maçã e 0 caso contrário.\n",
    "\n",
    "```\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -50 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "No projeto 1, experimentamos outras funções de reforço. No entanto, para os experimentos do projeto 2, decidimos manter sempre a `default_reward`, tendo em vista que apresentou bons resultados no projeto 1.\n",
    "\n",
    "## Parâmetros\n",
    "\n",
    "- **Quantidade de episódios de treinamento**: para comparar os métodos de forma justa, estabelecemos que todos os experimentos reportados seriam realizados com 1000 episódios de treinamento, assim como no projeto 1.\n",
    "\n",
    "- talvez ainda usemos isso - \n",
    "- N0\n",
    "\n",
    "- alfa\n",
    "\n",
    "- ver se tem algum parâmetro tipo o N0 ou o alfa e descrever aqui do que se trata -\n",
    "\n",
    "# Pontos-chave da implementação\n",
    "\n",
    "- falta escrever com uns pedaços importantes do código aqui\n",
    "\n",
    "## Particularidades e restrições da implementação\n",
    "\n",
    "Para executar o código, é preciso instalar a biblioteca PyGame. Isso pode ser feito com o comando `pip install pygame` no terminal.\n",
    "\n",
    "# Métodos\n",
    "\n",
    "## Método 1\n",
    "\n",
    "## Método 2 \n",
    "\n",
    "# Discussão de Resultados\n",
    "\n",
    "## Método 1 em relação a Método 2\n",
    "\n",
    "## Método 1 em relação a Projeto 1\n",
    "\n",
    "## Método 2 em relação a Projeto 1\n",
    "\n",
    "## Principais Conclusões\n",
    "\n",
    "# Observações\n",
    "\n",
    "Aqui neste notebook, colocamos exemplos de código para os principais experimentos. No entanto, todos os nossos experimentos estão em notebooks auxiliares usados por cada membro do grupo.\n",
    "\n",
    "- Notebook auxiliar 1 - experimentos com DQN realizados por Ana Clara Zoppi Serpa: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/ana_DQN.ipynb\n",
    "- Notebook auxiliar 2 - experimentos com A2C realizados por Gabriel Oliveira dos Santos: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/gabriel.ipynb\n",
    "- Notebook auxiliar 3 - experimentos com DQN realizados por Tito Barbosa Rezende: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/tito_DQN.ipynb\n",
    "- Notebook auxiliar 4 - experimentos com A2C realizados por Matteo Di Fabio: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/Matteo.ipynb\n",
    "\n",
    "Além disso, nossos resultados experimentais completos estão disponíveis na seguinte planilha: \n",
    "\n",
    "# Referências\n",
    "\n",
    "- atualizar as referências com outras coisas consultadas\n",
    "\n",
    "- Aulas da professora, disponibilizadas no Google Classroom da disciplina.\n",
    "\n",
    "- Livro Reinforcement Learning: An Introduction, Sutton and Barto, 2a Edição.\n",
    "\n",
    "### Artigos e exemplos de implementação\n",
    "\n",
    "- Artigo sobre Snake com Deep Q-Learning: https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n",
    "\n",
    "- Repositório do GitHub com Snake e Deep Q-Learning, no qual nos baseamos inicialmente para realizar o projeto: https://github.com/maurock/snake-ga\n",
    "\n",
    "- Exemplo de implementação do SARSA: https://www.geeksforgeeks.org/expected-sarsa-in-reinforcement-learning/\n",
    "\n",
    "- Artigo sobre SARSA(lambda) para vários jogos, inclusive Snake: http://cs229.stanford.edu/proj2012/JohnsonRobertsFisher-LearningToPlay2DVideoGames.pdf\n",
    "\n",
    "- Exemplo de implementação de Aproximador de Função: https://github.com/metastableB/Naagin-Naggin/blob/master/dlsnake/agents/approxQAgent.py\n",
    "\n",
    "- Exemplos de implementações de SARSA e Monte Carlo: https://github.com/ralhadeff/machine-learning-tools/tree/master/ReinforcementLearning\n",
    "\n",
    "- Exemplos de implementações baseados no livro Reinforcement Learning: An Introduction: https://github.com/flyywh/reinforcement-learning-1\n",
    "\n",
    "- Artigos sobre Snake, Q-Learning e SARSA: https://dkdennis.xyz/static/Nagging-report.pdf, http://cs229.stanford.edu/proj2016spr/report/060.pdf\n",
    "\n",
    "### Outras páginas\n",
    "\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
    "\n",
    "- https://towardsdatascience.com/slitherin-solving-the-classic-game-of-snake-with-ai-part-2-general-purpose-random-monte-25dc0dd4c4cf\n",
    "\n",
    "- https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566\n",
    "\n",
    "- https://towardsdatascience.com/monte-carlo-learning-b83f75233f92\n",
    "\n",
    "- https://medium.com/reinforcement-learning-a-step-by-step-implementati/reinforcement-learning-a-step-by-step-implementation-using-sarsa-1cfd3e64775a\n",
    "\n",
    "- https://www.programmersought.com/article/94202345056/\n",
    "\n",
    "- https://www.youtube.com/watch?v=l0sFUU7vScA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
